{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('input_file_wenhu.csv', mode='w') as fw:\n",
    "    with open('input_file_shiyang.csv', mode='w') as fs:\n",
    "        with open('input_file_yunkai.csv', mode='w') as fy:\n",
    "            \n",
    "            #output_wenhu = output[:5000]\n",
    "            #fileds = ['url1', 'q1', 'a1', 'url2', 'q2', 'a2', 'url3', 'q3', 'a3', 'url4', 'q4', 'a4', 'url5', 'q5', 'a5']\n",
    "            #writer = csv.DictWriter(fw, fieldnames=fileds)\n",
    "            #writer.writeheader()\n",
    "            #for i in range(0, len(output_wenhu) - 5, 5):\n",
    "            #    writer.writerow({'url1':output_wenhu[i]['url'], 'q1':output_wenhu[i]['q'], 'a1':output_wenhu[i]['a'],\n",
    "            #                    'url2':output_wenhu[i+1]['url'], 'q2':output_wenhu[i+1]['q'], 'a2':output_wenhu[i+1]['a'],\n",
    "            #                    'url3':output_wenhu[i+2]['url'], 'q3':output_wenhu[i+2]['q'], 'a3':output_wenhu[i+2]['a'],\n",
    "            #                    'url4':output_wenhu[i+3]['url'], 'q4':output_wenhu[i+3]['q'], 'a4':output_wenhu[i+3]['a'],\n",
    "            #                    'url5':output_wenhu[i+4]['url'], 'q5':output_wenhu[i+4]['q'], 'a5':output_wenhu[i+4]['a']})\n",
    "            \n",
    "            num = 10\n",
    "            \n",
    "            output_shiyang = output[5000:20000]\n",
    "            fields = []\n",
    "            for i in range(1, num+1):\n",
    "                fields.extend(['url{}'.format(i), 'q{}'.format(i), 'a{}'.format(i)])\n",
    "            writer = csv.DictWriter(fs, fieldnames=fields)\n",
    "            writer.writeheader()\n",
    "            for i in range(0, len(output_shiyang) - num, num):\n",
    "                elem = {}\n",
    "                for j in range(1, num+1):\n",
    "                    elem['url{}'.format(j)] = output_shiyang[i+j-1]['url']\n",
    "                    elem['q{}'.format(j)] = output_shiyang[i+j-1]['q']\n",
    "                    elem['a{}'.format(j)] = output_shiyang[i+j-1]['a']\n",
    "                \n",
    "                writer.writerow(elem) \n",
    "            \n",
    "            \n",
    "            output_yunkai = output[20000:]\n",
    "            fields = []\n",
    "            for i in range(1, num+1):\n",
    "                fields.extend(['url{}'.format(i), 'q{}'.format(i), 'a{}'.format(i)])\n",
    "            writer = csv.DictWriter(fy, fieldnames=fields)\n",
    "            writer.writeheader()\n",
    "            for i in range(0, len(output_yunkai) - num, num):\n",
    "                elem = {}\n",
    "                for j in range(1, num+1):\n",
    "                    elem['url{}'.format(j)] = output_yunkai[i+j-1]['url']\n",
    "                    elem['q{}'.format(j)] = output_yunkai[i+j-1]['q']\n",
    "                    elem['a{}'.format(j)] = output_yunkai[i+j-1]['a']\n",
    "                \n",
    "                writer.writerow(elem) \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex(inp):\n",
    "    inp = re.sub(r\"> +\", r\">\", inp)\n",
    "    inp = re.sub(r\" +<\", r\"<\", inp)\n",
    "    inp = re.sub(r\">@\", r\">\", inp)\n",
    "    inp = re.sub(r\"\\*\", r\"\", inp)\n",
    "    inp = re.sub(r\"#\", r\"\", inp)\n",
    "    inp = re.sub(r\"‡\", r\"\", inp)\n",
    "    #inp = re.sub(r\".0([^0-9])\", r\"\\1\", inp)\n",
    "    for month, abb in [(\"January\", \"Jan\"), (\"February\", \"Feb\"), (\"March\", \"March\"), (\"April\", \"April\"), \n",
    "                   (\"May\", \"May\"), (\"June\",\"June\") , (\"July\", \"July\"), (\"August\", \"Aug\"), \n",
    "                   (\"September\", \"Sep\"), (\"October\", \"Oct\"), (\"November\", \"Nov\"), (\"December\", \"Dec\")]:\n",
    "        inp = re.sub(r\"({})([0-9]+)\".format(month), r\"\\1 \\2\", inp)\n",
    "        inp = re.sub(r\"({})([0-9]+)\".format(abb), r\"\\1 \\2\", inp)\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "import jsonlines\n",
    "import csv\n",
    "import re\n",
    "\"\"\"\n",
    "with open('all_html_original.csv', 'w') as fw:\n",
    "    fields = ['url', 'content']\n",
    "    writer = csv.DictWriter(fw, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "\"\"\"\n",
    "with open('WikiTableQuestions/wikidata/train.tables.jsonl') as f:\n",
    "    data = jsonlines.Reader(f)\n",
    "    for d in data:\n",
    "        f = '<table class=\"wikitable\"><tr>'\n",
    "        for h in d['header']:\n",
    "            f += \"<th> {} </th>\".format(h)\n",
    "        f += \"</tr>\"\n",
    "        for r in d['rows']:\n",
    "            f += \"<tr>\"\n",
    "            for elem in r:\n",
    "                f += \"<td> {} </td>\".format(elem)\n",
    "            f += \"</tr>\"\n",
    "        f += \"</table>\"\n",
    "        #writer.writerow({'url': d['id'], \"content\": f})\n",
    "        with open('WikiTableQuestions/wikidata/all_html/{}.html'.format(d['id']), 'w') as fw:\n",
    "            print >> fw, regex(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def regex_equation(line):\n",
    "    line = re.sub(r\"([^+-])([0-9]+)-([0-9]+)-([0-9]+)-([0-9]+)=\", r\"\\1\\2+\\3+\\4+\\5=\", line)\n",
    "    line = re.sub(r\"([^+-])([0-9]+)-([0-9]+)-([0-9]+)=\", r\"\\1\\2+\\3+\\4=\", line)\n",
    "    line = re.sub(r\"([^+-])([0-9]+)-([0-9]+)=\", r\"\\1\\2+\\3=\", line)\n",
    "    #line = re.sub(r\"([^+-])([0-9]+)-([0-9]+)+([0-9]+)=\", r\"\\1\\2+\\3+\\4=\", line)\n",
    "    #line = re.sub(r\"([^+-])([0-9]+)-([0-9]+)-([0-9]+)-([0-9]+)=\", r\"\\1+\\2+\\3+\\4=\", line)    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_equation(\"67-54-69-68=270)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/tmp/list.txt') as f:\n",
    "    for line in f:\n",
    "        with open('WikiTableQuestions/wikidata/all_html/' + line.strip(), 'r') as f1:\n",
    "            content = f1.readline().strip()\n",
    "        with open('WikiTableQuestions/wikidata/all_html/' + line.strip(), 'w') as f1:\n",
    "            print >> f1, regex_equation(content)\n",
    "        #print regex_equation(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('WikiTableQuestions/wikidata/train_gold.json') as f:\n",
    "    raw_output = json.load(f)\n",
    "    output = [\"/\".join([str(__) for __ in _]) for _ in raw_output]\n",
    "\n",
    "with open('WikiTableQuestions/wikidata/all_training.tsv', 'w') as fw:\n",
    "    print >> fw, \"id\\tutterance\\tcontext\\ttargetValue\"\n",
    "    with open('WikiTableQuestions/wikidata/train.jsonl') as f:\n",
    "        data = jsonlines.Reader(f)\n",
    "        idx = 1\n",
    "        for d, o in zip(data, output):\n",
    "            print >> fw, \"{}\\t{}\\t{}\\t{}\".format(idx, d['question'].replace('\\t', ''), d['table_id'], o)\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "\n",
    "files = pandas.read_table('WikiTableQuestions/wikidata/all_training.tsv', delimiter=\"\\t\")\n",
    "\n",
    "output = []\n",
    "for f in os.listdir('WikiTableQuestions/wikidata/all_html/'):\n",
    "    if \"html\" in f:\n",
    "        numbering = f.split('.')[0]\n",
    "        results = files[files.context == numbering]\n",
    "        \n",
    "        for q, a in zip(results.utterance, results.targetValue):\n",
    "            tmp = {}\n",
    "            tmp[\"url\"] = 'https://raw.githubusercontent.com/wenhuchen/Interface/master/WikiTableQuestions/wikidata/all_html/{}.html'.format(numbering)\n",
    "            tmp[\"q\"] = q\n",
    "            tmp[\"a\"] = a\n",
    "            output.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('WikiTableQuestions/data/all_training_new.tsv', 'w') as fw:\n",
    "    with open('WikiTableQuestions/data/all_training.tsv') as f:\n",
    "        for line in f:\n",
    "            if \"csv\" in line:\n",
    "                try:\n",
    "                    t1, t2, t3, t4 = line.strip().split('\\t')\n",
    "                except Exception:\n",
    "                    print line.strip()\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    f1, f2, f3 = t3.split('/')\n",
    "                except Exception:\n",
    "                    print t3.strip()\n",
    "                    continue\n",
    "                    \n",
    "                t3 = f2.split('-')[0] + \"-\" + f3\n",
    "                new_line = \"\\t\".join([t1, t2, t3, t4])\n",
    "                print >> fw, new_line\n",
    "            else:\n",
    "                print >> fw, line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "result = pandas.read_table('WikiTableQuestions/wikidata/all_training.tsv')\n",
    "\n",
    "new_result = result[~result.targetValue.isin(['None', 'n/a', ])]\n",
    "\n",
    "new_result.to_csv('WikiTableQuestions/wikidata/all_training_new.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for r in result['Answer.d1']:\n",
    "    print r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas\n",
    "\n",
    "result = pandas.read_csv('results_v2.csv')\n",
    "filtered = result[result.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "items = []\n",
    "for i, r in filtered.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for i in range(1, num+1):\n",
    "        if r['Input.a{}'.format(i)] not in ['None', 'n/a'] and \" par \" not in r['Answer.d{}'.format(i)].lower():\n",
    "            items.append((r['Input.url{}'.format(i)], r['Input.q{}'.format(i)], r['Input.a{}'.format(i)], r['Answer.d{}'.format(i)]))\n",
    "\n",
    "with open('v2_results.json', 'w') as f:\n",
    "    json.dump(items, f, indent=2)\n",
    "    \n",
    "with open('v2_rewrite_input.csv', 'w') as f:\n",
    "    #json.dump(items, f, indent=2)\n",
    "    fields = []\n",
    "    for j in range(1, num + 1):\n",
    "        fields.append(\"url{}\".format(j))\n",
    "        fields.append(\"s{}\".format(j))\n",
    "        \n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for i in range(0, len(items) - num, num):\n",
    "        elem = {}\n",
    "        for j in range(num):\n",
    "            elem[\"url{}\".format(j + 1)] = items[i + j][0]\n",
    "            elem[\"s{}\".format(j + 1)] = items[i + j][3]\n",
    "            \n",
    "        writer.writerow(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "\n",
    "with open('v2_results.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "keys = [_[1] for _ in data]\n",
    "result = pandas.read_csv('results_v2_new.csv')\n",
    "filtered = result[result.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "items = []\n",
    "for i, r in filtered.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "        \n",
    "    for i in range(1, num+1):\n",
    "        if r['Input.a{}'.format(i)] not in ['None', 'n/a'] and \" par \" not in r['Answer.d{}'.format(i)].lower() and r['Input.q{}'.format(i)] not in keys:\n",
    "            items.append((r['Input.url{}'.format(i)], r['Input.q{}'.format(i)], r['Input.a{}'.format(i)], r['Answer.d{}'.format(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "result = pandas.read_csv('results_v3.csv')\n",
    "\n",
    "filtered = result[result.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "new_items = []\n",
    "for i, r in filtered.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for i in range(1, num + 1):\n",
    "        if r['Input.a{}'.format(i)] not in ['None', 'n/a'] and \" par \" not in str(r['Answer.d{}'.format(i)]).lower():\n",
    "            new_items.append((r['Input.url{}'.format(i)], r['Input.q{}'.format(i)], r['Input.a{}'.format(i)], r['Answer.d{}'.format(i)]))\n",
    "\n",
    "new_items = new_items + items\n",
    "\n",
    "with open('v3_results.json', 'w') as f:\n",
    "    json.dump(new_items, f, indent=2)\n",
    "    \n",
    "num = 5\n",
    "with open('v3_rewrite_input.csv', 'w') as f:\n",
    "    #json.dump(items, f, indent=2)\n",
    "    fields = []\n",
    "    for j in range(1, num + 1):\n",
    "        fields.append(\"url{}\".format(j))\n",
    "        fields.append(\"s{}\".format(j))\n",
    "        \n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for i in range(0, len(new_items) - num, num):\n",
    "        elem = {}\n",
    "        for j in range(num):\n",
    "            elem[\"url{}\".format(j + 1)] = new_items[i + j][0]\n",
    "            elem[\"s{}\".format(j + 1)] = new_items[i + j][3]\n",
    "            \n",
    "        writer.writerow(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(new_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = \"\"\n",
    "for month, abb in [(\"January\", \"Jan\"), (\"February\", \"Feb\"), (\"March\", \"March\"), (\"April\", \"April\"), \n",
    "                   (\"May\", \"May\"), (\"June\",\"June\") , (\"July\", \"July\"), (\"August\", \"Aug\"), \n",
    "                   (\"September\", \"Sep\"), (\"October\", \"Oct\"), (\"November\", \"Nov\"), (\"December\", \"Dec\")]:\n",
    "    s += \";s/({})([1-9]+)/\\\\1 \\\\2/g;s/({}),([1-9]+)//g\".format(month, abb)\n",
    "print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import simplejson\n",
    "\n",
    "result = pandas.read_csv('v2_refine.csv')\n",
    "filtered_result = result[result.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "items = []\n",
    "num = 10\n",
    "for i, r in filtered_result.iterrows():\n",
    "    for i in range(1, num+1):\n",
    "        items.append((r[\"Input.url{}\".format(i)],\"-\" ,\"-\" , r[\"Answer.d{}\".format(i)]))\n",
    "\n",
    "with open('v2_rewrite.json', 'w') as f:    \n",
    "    simplejson.dump(items, f, encoding='utf-8', ignore_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#import pandas\n",
    "import re\n",
    "import csv\n",
    "\n",
    "def dealwithNum(inp):\n",
    "    inp = re.sub(r\"([^0-9.])(0)([0-9])%\", r\"\\1\\2.\\3%\", inp)\n",
    "    \n",
    "    inp = re.sub(r\"([^0-9.])([1-9])([0-9])([0-9])%\", r\"\\1\\2\\3.\\4%\", inp)\n",
    "    inp = re.sub(r\"([^0-9.])(0)([0-9])([0-9])%\", r\"\\1\\2.\\3\\4%\", inp)\n",
    "    \n",
    "    inp = re.sub(r\"([^0-9.])([1-9])([0-9])([0-9])([0-9])%\", r\"\\1\\2\\3.\\4\\5%\", inp)\n",
    "    inp = re.sub(r\"([^0-9.])(0)([0-9])([0-9])([0-9])%\", r\"\\1\\2.\\3\\4\\5%\", inp)\n",
    "    \n",
    "    inp = re.sub(r\"10000%\", r\"100.00%\", inp)\n",
    "    inp = re.sub(r\"1000%\", r\"100.0%\", inp)\n",
    "    \n",
    "    inp = re.sub(r\"([^0-9.])([1-9])([0-9])([0-9])([0-9])([0-9])%\", r\"\\1\\2\\3.\\4\\5\\6%\", inp)\n",
    "    inp = re.sub(r\"([^0-9.])(0)([0-9])([0-9])([0-9])([0-9])%\", r\"\\1\\2.\\3\\4\\5\\6%\", inp)\n",
    "    \n",
    "    inp = re.sub(r\",([0-9])%\", r\".\\1%\", inp)\n",
    "    inp = re.sub(r\",([0-9])([0-9])%\", r\".\\1\\2%\", inp)\n",
    "    inp = re.sub(r\",([0-9])([0-9])([0-9])%\", r\".\\1\\2\\3%\", inp)\n",
    "    \n",
    "    \n",
    "    #inp = re.sub(r\"([0-9]),([0-9])\", r\"\\1\\2\", inp)\n",
    "    return inp\n",
    "\n",
    "\n",
    "with open('all_html.csv', 'w') as f:\n",
    "    fields = [\"url\", \"content\"]\n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for f in os.listdir('WikiTableQuestions/wikidata/all_html/'):\n",
    "        with open('WikiTableQuestions/wikidata/all_html/' + f, 'r') as fr:\n",
    "            string = fr.readline().strip()\n",
    "        \n",
    "        writer.writerow({\"url\": f, \"content\": dealwithNum(string)})\n",
    "\n",
    "#inp_s = \"<td>123% <td> 0123% <td>012% <td>1233%  <td>12333% <td>10000% <td>01333%\"\n",
    "#dealwithNum(inp_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "table = pandas.read_csv('all_html.csv')\n",
    "\n",
    "for i, item in table.iterrows():\n",
    "    name = item['url']\n",
    "    content = item['content']\n",
    "    with open('WikiTableQuestions/wikidata/all_html/{}'.format(name), 'w') as f:\n",
    "        print >> f, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas\n",
    "\n",
    "result = pandas.read_csv('results_v4_1.csv')\n",
    "\n",
    "filtered = result[result.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "items = []\n",
    "for i, r in filtered.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for i in range(1, num+1):\n",
    "        if r['Input.a{}'.format(i)] not in ['None', 'n/a'] and \" par \" not in r['Answer.d{}'.format(i)].lower():\n",
    "            items.append((r['Input.url{}'.format(i)], r['Input.q{}'.format(i)], r['Input.a{}'.format(i)], r['Answer.d{}'.format(i)]))\n",
    "\n",
    "with open('v4_1_results.json', 'w') as f:\n",
    "    json.dump(items, f, indent=2)\n",
    "\n",
    "num = 5\n",
    "with open('v4_rewrite_input.csv', 'w') as f:\n",
    "    #json.dump(items, f, indent=2)\n",
    "    fields = []\n",
    "    for j in range(1, num + 1):\n",
    "        fields.append(\"url{}\".format(j))\n",
    "        fields.append(\"s{}\".format(j))\n",
    "    \n",
    "    #fields.extend([\"url11\", \"s11\"])\n",
    "    \n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for i in range(0, len(items) - num, num):\n",
    "        elem = {}\n",
    "        for j in range(num):\n",
    "            elem[\"url{}\".format(j + 1)] = items[i + j][0]\n",
    "            elem[\"s{}\".format(j + 1)] = items[i + j][3]\n",
    "        \n",
    "        #elem['url11'] = \"https://raw.githubusercontent.com/wenhuchen/Interface/master/WikiTableQuestions/wikidata/all_html/2-1236260-1.html\"\n",
    "        #elem['s11'] = \"Total is the rank of total\"\n",
    "            \n",
    "        writer.writerow(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import csv\n",
    "import json\n",
    "import simplejson\n",
    "\n",
    "r1 = pandas.read_csv('partial_refine_v2.csv')\n",
    "r2 = pandas.read_csv('partial_refine_v3.csv')\n",
    "r1 = r1[r1.AssignmentStatus==\"Approved\"]\n",
    "r2 = r2[r2.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "results = {}\n",
    "finished = []\n",
    "for i, r in r1.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        html_name = r['Input.url{}'.format(j)].split('/')[-1]\n",
    "        t = r['Answer.d{}'.format(j)]\n",
    "        if t and isinstance(t, str) and t.lower() not in [\"na\", \"none\", \"n/a\", \"no\"]:\n",
    "            if html_name not in results:\n",
    "                results[html_name] = {\"text\": [], \"label\": []}\n",
    "            results[html_name]['text'].append(t)\n",
    "            results[html_name]['label'].append(1)\n",
    "        finished.append(r['Input.s{}'.format(j)])\n",
    "        \n",
    "for i, r in r2.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        html_name = r['Input.url{}'.format(j)].split('/')[-1]\n",
    "        t = r['Answer.d{}'.format(j)]\n",
    "        if t and isinstance(t, str) and t.lower() not in [\"na\", \"none\", \"n/a\", \"no\"]:\n",
    "            if html_name not in results:\n",
    "                results[html_name] = {\"text\": [], \"label\": []}        \n",
    "            results[html_name]['text'].append(r['Answer.d{}'.format(j)])\n",
    "            results[html_name]['label'].append(1)\n",
    "        finished.append(r['Input.s{}'.format(j)])\n",
    "\n",
    "r1 = pandas.read_csv('partial_neg_v2.csv')\n",
    "r2 = pandas.read_csv('partial_neg_v3.csv')\n",
    "r1 = r1[r1.AssignmentStatus==\"Approved\"]\n",
    "r2 = r2[r2.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "for i, r in r1.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        t = r['Answer.d{}'.format(j)]\n",
    "        if t and isinstance(t, str) and t.lower() not in [\"na\", \"none\", \"n/a\", \"no\"]:\n",
    "            html_name = r['Input.url{}'.format(j)].split('/')[-1]\n",
    "            if html_name not in results:\n",
    "                results[html_name] = {\"text\": [], \"label\": []}\n",
    "            if r['Answer.d{}'.format(j)] not in results[html_name]['text']:\n",
    "                results[html_name]['text'].append(r['Answer.d{}'.format(j)])\n",
    "                results[html_name]['label'].append(-1)\n",
    "\n",
    "for i, r in r2.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        t = r['Answer.d{}'.format(j)]\n",
    "        if t and isinstance(t, str) and t.lower() not in [\"na\", \"none\", \"n/a\", \"no\"]:\n",
    "            html_name = r['Input.url{}'.format(j)].split('/')[-1]\n",
    "            if html_name not in results:\n",
    "                results[html_name] = {\"text\": [], \"label\": []}        \n",
    "            results[html_name]['text'].append(r['Answer.d{}'.format(j)])\n",
    "            results[html_name]['label'].append(-1)\n",
    "            \n",
    "with open('READY/prelim.json', 'w') as f:\n",
    "    simplejson.dump(results, f, encoding='utf-8', ignore_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results\n",
    "inp_v2 = pandas.read_csv('v2_rewrite_input.csv')\n",
    "inp_v3 = pandas.read_csv('v3_rewrite_input.csv')\n",
    "\n",
    "not_finished = []\n",
    "done = 0\n",
    "for i, r in inp_v2.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        if r['s{}'.format(j)] not in finished:\n",
    "            not_finished.append((r['url{}'.format(j)], r['s{}'.format(j)]))\n",
    "        else:\n",
    "            done += 1\n",
    "        \n",
    "for i, r in inp_v3.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        if r['s{}'.format(j)] not in finished:\n",
    "            not_finished.append((r['url{}'.format(j)], r['s{}'.format(j)]))\n",
    "        else:\n",
    "            done += 1\n",
    "\n",
    "num = 5\n",
    "with open('v23_left_rewrite_input.csv', 'w') as f:\n",
    "    #json.dump(items, f, indent=2)\n",
    "    fields = []\n",
    "    for j in range(1, num + 1):\n",
    "        fields.append(\"url{}\".format(j))\n",
    "        fields.append(\"s{}\".format(j))\n",
    "    \n",
    "    #fields.extend([\"url11\", \"s11\"])\n",
    "    \n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for i in range(0, len(not_finished) - num, num):\n",
    "        elem = {}\n",
    "        for j in range(num):\n",
    "            elem[\"url{}\".format(j + 1)] = not_finished[i + j][0]\n",
    "            elem[\"s{}\".format(j + 1)] = not_finished[i + j][1]\n",
    "        writer.writerow(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pandas.read_csv(\"input_file_yunkai.csv\")\n",
    "keys = set()\n",
    "for i, r in result.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        keys.add(r[\"url{}\".format(j)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pandas.read_csv(\"v4_rewrite_input.csv\")\n",
    "#keys = set()\n",
    "for i, r in result.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        if r[\"url{}\".format(j)] in keys:\n",
    "            keys.remove(r[\"url{}\".format(j)])\n",
    "unseen_tables = list(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('READY/prelim.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "pos_length = 0\n",
    "neg_length = 0\n",
    "for k in data:\n",
    "    text = data[k]['text']\n",
    "    label = data[k]['label']\n",
    "    for t, l in zip(text, label):\n",
    "        if l == 1:\n",
    "            pos.append(t)\n",
    "            pos_length += len(t.split())\n",
    "        else:\n",
    "            neg.append(t)\n",
    "            neg_length += len(t.split())\n",
    "\n",
    "print len(pos), len(neg)\n",
    "print (pos_length + 0.0) / len(pos)\n",
    "print (neg_length + 0.0) / len(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "a = Counter()\n",
    "a.update([1,2,3])\n",
    "a.update([1,3,4])\n",
    "\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import csv\n",
    "\n",
    "r1 = pandas.read_csv('/Users/wenhuchen/Downloads/Batch_3580679_batch_results.csv')\n",
    "r2 = pandas.read_csv('/Users/wenhuchen/Downloads/Batch_3584639_batch_results.csv')\n",
    "r1 = r1[r1.AssignmentStatus==\"Approved\"]\n",
    "r2 = r2[r2.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "r3 = pandas.concat([r1, r2])\n",
    "finished = []\n",
    "\n",
    "r1_inp = pandas.read_csv('v23_left_rewrite_input.csv')\n",
    "r2_inp = pandas.read_csv('v4_rewrite_input.csv')\n",
    "r3_inp = pandas.concat([r1_inp, r2_inp])\n",
    "\n",
    "for i, r in r3.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        #if r[\"url{}\".format(j)] in keys:\n",
    "        finished.append(r[\"Input.s{}\".format(j)])\n",
    "\n",
    "print len(finished)\n",
    "unfinished = []\n",
    "done = 0\n",
    "finished_old = []\n",
    "for i,r in r3_inp.iterrows():\n",
    "    if \"url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        #if r[\"url{}\".format(j)] in keys:\n",
    "        if r[\"s{}\".format(j)]  not in finished:\n",
    "            unfinished.append((r[\"url{}\".format(j)], r[\"s{}\".format(j)]))\n",
    "        else:\n",
    "            finished_old.append(r[\"s{}\".format(j)])\n",
    "            done += 1\n",
    "\n",
    "print done\n",
    "with open('v234_rest_rewrite_input.csv', 'w') as f:\n",
    "    #json.dump(items, f, indent=2)\n",
    "    num = 5\n",
    "    fields = []\n",
    "    for j in range(1, num + 1):\n",
    "        fields.append(\"url{}\".format(j))\n",
    "        fields.append(\"s{}\".format(j))\n",
    "        \n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for i in range(0, len(unfinished) - num, num):\n",
    "        elem = {}\n",
    "        for j in range(num):\n",
    "            elem[\"url{}\".format(j + 1)] = unfinished[i + j][0]\n",
    "            elem[\"s{}\".format(j + 1)] = unfinished[i + j][1]\n",
    "            \n",
    "        writer.writerow(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r1 = pandas.read_csv('/Users/wenhuchen/Downloads/Batch_3583922_batch_results.csv')\n",
    "r2 = pandas.read_csv('/Users/wenhuchen/Downloads/Batch_3579309_batch_results.csv')\n",
    "r3 = pandas.read_csv('/Users/wenhuchen/Downloads/Batch_3579785_batch_results.csv')\n",
    "\n",
    "r1 = r1[r1.AssignmentStatus==\"Approved\"]\n",
    "r2 = r2[r2.AssignmentStatus==\"Approved\"]\n",
    "r3 = r3[r3.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "r4 = pandas.concat([r1, r2, r3])\n",
    "finished = []\n",
    "\n",
    "\n",
    "for i, r in r4.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        finished.append(r[\"Input.s{}\".format(j)])\n",
    "print len(finished)\n",
    "\n",
    "r1_inp = pandas.read_csv('v2_rewrite_input.csv')\n",
    "r2_inp = pandas.read_csv('v3_rewrite_input.csv')\n",
    "r3_inp = pandas.read_csv('v4_rewrite_input.csv')\n",
    "r4_inp = pandas.concat([r1_inp, r2_inp, r3_inp])\n",
    "unfinished = []\n",
    "done = 0\n",
    "for i,r in r4_inp.iterrows():\n",
    "    if \"url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        #if r[\"url{}\".format(j)] in keys:\n",
    "        if r[\"s{}\".format(j)]  not in finished:\n",
    "            unfinished.append((r[\"url{}\".format(j)], r[\"s{}\".format(j)]))\n",
    "        else:\n",
    "            done += 1\n",
    "\n",
    "print done\n",
    "with open('v234_rest_rewrite_fake_input.csv', 'w') as f:\n",
    "    num = 5\n",
    "    fields = []\n",
    "    for j in range(1, num + 1):\n",
    "        fields.append(\"url{}\".format(j))\n",
    "        fields.append(\"s{}\".format(j))\n",
    "        \n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for i in range(0, len(unfinished) - num, num):\n",
    "        elem = {}\n",
    "        for j in range(num):\n",
    "            elem[\"url{}\".format(j + 1)] = unfinished[i + j][0]\n",
    "            elem[\"s{}\".format(j + 1)] = unfinished[i + j][1]\n",
    "            \n",
    "        writer.writerow(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def substitute(string):\n",
    "    string = string.lower()\n",
    "    \n",
    "    string = string.replace(r\"\\n\", \"\")\n",
    "    string = string.replace(r\"\\\\\", \"\")\n",
    "    string = string.replace(r\"!$\", \"\")\n",
    "    string = string.replace(r\"#\", \"\")\n",
    "    string = string.replace('–', '-')\n",
    "    string = string.replace('−', '-')\n",
    "    string = string.replace('−', '-')\n",
    "    string = string.replace('â', '')\n",
    "    string = string.replace('“', '')\n",
    "    string = string.replace('€', '')    \n",
    "    string = string.replace(\"√\", \"\")\n",
    "    string = string.replace(r'\"', '')\n",
    "    string = string.replace(r'\\.0 ', '')\n",
    "    string = string.replace(r'¹', '')\n",
    "    \n",
    "    string = string.replace(r\"'s\", \" 's\")\n",
    "    string = string.replace(r\"'re\", \" 're\")\n",
    "    string = string.replace(r\"'nt\", \" not\")\n",
    "    string = string.replace(r'@', ' ')\n",
    "    string = string.replace(r'`', '')\n",
    "    string = string.replace(r'•', ' ')\n",
    "    string = string.replace(r'·', ' ')\n",
    "    string = string.replace(r'²', ' square')\n",
    "    string = string.replace(r'\\[', '')\n",
    "    string = string.replace(r'\\]', '')\n",
    "    string = string.replace(r'\\{', '')\n",
    "    string = string.replace(r'\\}', '')\n",
    "    string = string.replace(r'\\|', '')\n",
    "    string = string.replace(r'\\^', '')\n",
    "\n",
    "    string = re.sub(r'\\[.+\\]', ' ', string)\n",
    "    string = re.sub(r',([0-9]{3})(?![0-9])', r'\\1', string)\n",
    "    #string = re.sub(r'([0-9]{1-3}),([0-9]{3}),([0-9]{3})([^0-9])', r\"\\1\\2\\3\\4\", string)\n",
    "    #string = re.sub(r'([0-9]{1-3}),([0-9]{3}),([0-9]{3}),([0-9]{3})([^0-9])', r\"\\1\\2\\3\\4\\5\", string)\n",
    "    #string = re.sub(r'([0-9]{1-3}),([0-9]{3}),([0-9]{3}),([0-9]{3}),([0-9]{3})([^0-9])', r\"\\1\\2\\3\\4\\5\\6\", string)\n",
    "    \n",
    "    string = re.sub(r'([^,]),', r\"\\1 ,\", string)\n",
    "    string = re.sub(r',([^,])', r\", \\1\", string)\n",
    "    string = re.sub(r'([^(])\\(', r\"\\1 (\", string)\n",
    "    string = re.sub(r'\\)([^)])', r\") \\1\", string)\n",
    "    string = re.sub(r'\\.\\)', r\")\", string)\n",
    "    string = re.sub(r'([0-9]+)-([0-9]+)-([0-9]+)-([0-9]+)=', r\"\\1+\\2+\\3+\\4=\", string)\n",
    "    string = re.sub(r'([0-9]+)-([0-9]+)-([0-9]+)=', r\"\\1+\\2+\\3=\", string)\n",
    "    string = re.sub(r'([0-9]+)-([0-9]+)=', r\"\\1+\\2=\", string)\n",
    "    string = re.sub(r\"' +([0-9])\", r\"'\\1\", string)\n",
    "\n",
    "    string = re.sub(r\"([^ ])\\+\", r\"\\1 +\", string)\n",
    "    string = re.sub(r\"\\+([^ ])\", r\"+ \\1\", string)\n",
    "    string = re.sub(r\"([^ ])=\", r\"\\1 =\", string)\n",
    "    string = re.sub(r\"=([^ ])\", r\"= \\1\", string)\n",
    "\n",
    "    string = re.sub(r\"-([^- ])\", r\"- \\1\", string)\n",
    "    string = re.sub(r\"([^- ])-\", r\"\\1 -\", string)\n",
    "    string = re.sub(r\"-([^- ])\", r\"- \\1\", string)\n",
    "    string = re.sub(r\"/([^ ])\", r\"/ \\1\", string)\n",
    "    string = re.sub(r\"([^ ])/\", r\"\\1 /\", string)\n",
    "    \n",
    "    string = re.sub(r'([^0-9 ]):', r\"\\1 :\", string)\n",
    "    string = re.sub(r':([^0-9 ])', r\": \\1\", string)\n",
    "    string = re.sub(r'([0-9])pm', r\"\\1 pm\", string)\n",
    "    string = re.sub(r'([0-9])am', r\"\\1 am\", string)\n",
    "    string = re.sub(r'([0-9])rpm', r\"\\1 rpm\", string)\n",
    "    string = re.sub(r'([0-9])km', r\"\\1 km\", string)\n",
    "    string = re.sub(r'([0-9])cm', r\"\\1 cm\", string)\n",
    "    string = re.sub(r'([0-9])m', r\"\\1 m\", string)\n",
    "    string = re.sub(r'([0-9])mm', r\"\\1 mm\", string)\n",
    "    string = re.sub(r'([0-9])kg', r\"\\1 kg\", string)\n",
    "    string = re.sub(r'([0-9])g', r\"\\1 g\", string)\n",
    "    string = re.sub(r'([0-9])kw', r\"\\1 kw\", string)\n",
    "    string = re.sub(r'([0-9])kv', r\"\\1 kv\", string)\n",
    "    string = re.sub(r'([0-9])mph', r\"\\1 mph\", string)\n",
    "    #string = re.sub(r'([0-9])@', r\"\\1 @\", string)\n",
    "    #string = re.sub(r'@([0-9])', r\"@ \\1\", string)\n",
    "    string = re.sub(r'category : articles with hcards', r\"\", string)\n",
    "    string = re.sub(r'category : articles with hcard', r\"\", string)\n",
    "    string = re.sub(r'category : articles without hcard', r\"\", string)\n",
    "    \n",
    "    string = re.sub(r\"\\.+$\", '', string)\n",
    "    string = re.sub(r',+$', '', string)    \n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    string = re.sub(r'^ ', '', string)\n",
    "    string = re.sub(r' $', '', string)\n",
    "    string = re.sub('70 - 76 - 68 - 214', '70 + 76 + 68 = 214', string)\n",
    "    return string\n",
    "\n",
    "#print substitute(\"fa\\g##ga,/// she got(ff)ff...\")\n",
    "#print substitute('(5) fa:\"12-12-13=31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blacklist(string):\n",
    "    black = ['bye', \"no chart for\", \"km (mi)\", \"lb·ft\", \"ft (m)\", \"kg (lb)\", \n",
    "             \"a report of report\", \"report was report\", \"is 'report'\", \"?\"]\n",
    "    for b in black:\n",
    "        if b in string:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "with open('data/short_subset.txt') as f:\n",
    "    limit_length = [_.strip() for _ in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "import pandas\n",
    "\n",
    "r1_1 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/refine_0.csv\")\n",
    "r1_2 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/refine_1.csv\")\n",
    "\n",
    "r1 = pandas.concat([r1_1, r1_2])\n",
    "r1 = r1[r1.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "f = open('/tmp/output.txt', 'w')\n",
    "\n",
    "index = 0\n",
    "finished = {}\n",
    "trash = []\n",
    "for i,r in r1.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        orig_input = r[\"Answer.d{}\".format(j)]\n",
    "        html_name = r['Input.url{}'.format(j)].split('/')[-1] + '.csv'\n",
    "        if html_name not in limit_length:\n",
    "            continue\n",
    "        if isinstance(orig_input, str) and orig_input.lower() in [\"na\", \"n/a\", \"no\"]:\n",
    "            #trash.append(html_name)\n",
    "            print \"error\"\n",
    "        if isinstance(orig_input, str) and len(orig_input.split(' ')) > 3 and not blacklist(orig_input):\n",
    "            replaced_sent = substitute(orig_input)\n",
    "            if 'hcard' not in replaced_sent:\n",
    "                print >> f, replaced_sent\n",
    "                index += 1\n",
    "                if html_name not in finished:\n",
    "                    finished[html_name] = [[replaced_sent], [1]]\n",
    "                else:\n",
    "                    finished[html_name][0].append(replaced_sent)\n",
    "                    finished[html_name][1].append(1)\n",
    "                    \n",
    "r2_1 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/refine_2.csv\")\n",
    "r2_2 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/refine_3.csv\")\n",
    "r2_3 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/refine_4.csv\")\n",
    "\n",
    "r2 = pandas.concat([r2_1, r2_2, r2_3])\n",
    "r2 = r2[r2.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "for i,r in r2.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        orig_input = r[\"Answer.d{}\".format(j)]\n",
    "        html_name = r['Input.url{}'.format(j)].split('/')[-1] + '.csv'\n",
    "        if html_name not in limit_length:\n",
    "            continue     \n",
    "        if isinstance(orig_input, str) and orig_input.lower() in [\"na\", \"n/a\", \"no\"]:\n",
    "            print \"error\"\n",
    "        if isinstance(orig_input, str) and len(orig_input.split(' ')) > 3 and not blacklist(orig_input):\n",
    "            replaced_sent = substitute(orig_input)\n",
    "            if 'hcard' not in replaced_sent:\n",
    "                print >> f, replaced_sent\n",
    "                index += 1\n",
    "                if html_name not in finished:\n",
    "                    finished[html_name] = [[replaced_sent], [1]]\n",
    "                else:\n",
    "                    finished[html_name][0].append(replaced_sent)\n",
    "                    finished[html_name][1].append(1)\n",
    "\n",
    "\n",
    "r3_1 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/fake_0.csv\")\n",
    "r3_2 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/fake_1.csv\")\n",
    "r3_3 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/fake_2.csv\")\n",
    "r3_4 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/fake_3.csv\")\n",
    "\n",
    "r3 = pandas.concat([r3_1, r3_2, r3_3, r3_4])\n",
    "r3 = r3[r3.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "f = open('/tmp/output.txt', 'w')\n",
    "\n",
    "index = 0\n",
    "for i,r in r3.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        orig_input = r[\"Answer.d{}\".format(j)]\n",
    "        html_name = r['Input.url{}'.format(j)].split('/')[-1] + '.csv'\n",
    "        if html_name not in limit_length:\n",
    "            continue\n",
    "        if isinstance(orig_input, str) and orig_input.lower() in [\"na\", \"n/a\", \"no\"]:\n",
    "            print \"error\"\n",
    "        if isinstance(orig_input, str) and len(orig_input.split(' ')) > 3 and not blacklist(orig_input):\n",
    "            replaced_sent = substitute(orig_input)\n",
    "            if 'hcard' not in replaced_sent:\n",
    "                print >> f, replaced_sent\n",
    "                index += 1\n",
    "                if html_name not in finished:\n",
    "                    finished[html_name] = [[replaced_sent], [0]]\n",
    "                else:\n",
    "                    finished[html_name][0].append(replaced_sent)\n",
    "                    finished[html_name][1].append(0)\n",
    "\n",
    "f.close()\n",
    "print index\n",
    "import json\n",
    "with open(\"READY/training_all.json\", 'w') as f:\n",
    "    json.dump(finished, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print trash\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print trash\n",
    "for l in trash + [\"2-11916083-12.html\", \"2-1236260-1.html\"]:\n",
    "    try:\n",
    "        shutil.move(\"data/all_csv/\" + l + \".csv\", \"data/trash_csv/\" + l + \".csv\")\n",
    "    except Exception:\n",
    "        print \"error, {}\".format(\"data/all_csv/\" + l + \".csv\", \"data/trash_csv/\" + l + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import csv\n",
    "\n",
    "url = open('WikiTableQuestions/wikidata/all_html/2-18178551-5.html').readline().strip()\n",
    "soup = BeautifulSoup(url, \"html.parser\")\n",
    "\n",
    "table = soup.findAll(\"table\", {\"class\":\"wikitable\"})[0]\n",
    "rows = table.findAll(\"tr\")\n",
    "\n",
    "with open('/tmp/editors.csv', \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in rows:\n",
    "        csv_row = []\n",
    "        for cell in row.findAll([\"td\", \"th\"]):\n",
    "            csv_row.append(cell.get_text())\n",
    "        writer.writerow(csv_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "import csv\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "for filename in os.listdir('WikiTableQuestions/wikidata/all_html/'):\n",
    "    if filename.endswith(\".html\"):\n",
    "        url = open('WikiTableQuestions/wikidata/all_html/' + filename).readline().strip()\n",
    "        soup = BeautifulSoup(url, \"html.parser\")\n",
    "        table = soup.findAll(\"table\", {\"class\":\"wikitable\"})\n",
    "        if len(table) > 0:\n",
    "            table = table[0]\n",
    "            rows = table.findAll(\"tr\")\n",
    "            with open('WikiTableQuestions/wikidata/all_csv/' + filename + '.csv', 'w') as csvfile:\n",
    "                spamwriter = csv.writer(csvfile, delimiter='#')\n",
    "                for row in rows:\n",
    "                    csv_row = []\n",
    "                    for cell in row.findAll([\"td\", \"th\"]):\n",
    "                        print cell.get_text()\n",
    "                        csv_row.append(substitute(cell.get_text()))\n",
    "                        print substitute(cell.get_text())\n",
    "                        print \"\"\n",
    "                    spamwriter.writerow(csv_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "import csv\n",
    "import urllib\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open('READY/training_all.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "files = data.keys()\n",
    "\n",
    "unseen = []\n",
    "with open('data/short_subset.txt') as fs:\n",
    "    for f in fs:\n",
    "        f = f.strip()\n",
    "        if f not in files and f in tiny_mapping:\n",
    "            unseen.append(f)\n",
    "with open(\"data/v5_unseen.json\", 'w') as f:\n",
    "    json.dump(unseen, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "import json\n",
    "import urllib\n",
    "import json\n",
    "import os \n",
    "import csv\n",
    "\n",
    "with open('data/table_to_page_new.json', 'r') as f:\n",
    "    tiny_mapping = json.load(f)\n",
    "    \n",
    "with open(\"data/v5_unseen.json\", 'r') as f:\n",
    "    unseen = json.load(f)\n",
    "    \n",
    "with open('v5_write_input.csv', 'w') as f:\n",
    "    fields = [\"url1\", \"wiki1\", \"topic1\"]\n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for k in unseen:\n",
    "        if k in tiny_mapping:\n",
    "            writer.writerow({\"url1\": 'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/' + k, \n",
    "                            \"wiki1\": tiny_mapping[k][1], \"topic1\": tiny_mapping[k][0]})\n",
    "        else:\n",
    "            writer.writerow({\"url1\": 'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/' + k, \"wiki1\": \"javascript:void(0)\", \"topic1\": \"None\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/tmp/table_to_page.json', 'r') as f:\n",
    "    old_tiny_mapping1 = json.load(f)\n",
    "with open('data/table_to_page.json', 'r') as f:\n",
    "    old_tiny_mapping2 = json.load(f)\n",
    "    \n",
    "new_tiny_mapping = {}\n",
    "for k, v in old_tiny_mapping2.iteritems():\n",
    "    new_tiny_mapping[k] = [substitute(old_tiny_mapping1[k].split('/')[-1].replace('_', ' ')), old_tiny_mapping2[k]]\n",
    "\n",
    "with open('data/table_to_page_new.json', 'w') as f:\n",
    "    json.dump(new_tiny_mapping, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/tmp/g.txt', 'w') as f:\n",
    "    x = \"https://en.wikipedia.org/wiki/2007%E2%80%9308_Scottish_Second_Division\"\n",
    "    print >> f, [\"topic\", urllib.unquote(x).split('/')[-1]]\n",
    "    x = 'https://en.wikipedia.org/wiki/Ana_Timoti%C4%87'\n",
    "    print >> f, {\"topic\": urllib.unquote(x).split('/')[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "name = files[0]\n",
    "tab = pandas.read_csv('data/all_csv/' + name + '.csv', delimiter='#')\n",
    "sent = data[name][0][0]\n",
    "label = data[name][1][0]\n",
    "\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print sent, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "activations = []\n",
    "for i, t in enumerate(tab.columns):\n",
    "    for j, v in enumerate(tab[t]):\n",
    "        if isinstance(v, str) and len(v.split(' ')) > 2:\n",
    "            if fuzz.partial_ratio(v, sent) > 95:\n",
    "                print t, j\n",
    "        else:\n",
    "            if str(v) in sent:\n",
    "                print t, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from collections import Counter\n",
    "import pprint\n",
    "\n",
    "t = pandas.read_csv(\"/Users/wenhuchen/Downloads/Batch_3598504_batch_results.csv\")\n",
    "t = t[t.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "\n",
    "def transform(string):\n",
    "    if string == \"Problematic\":\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "kv_pairs = {}\n",
    "num = 5\n",
    "for i, r in t.iterrows():\n",
    "    for j in range(1, num + 1):\n",
    "        csv_name = r['Input.url{}'.format(j)].split('/')[-1]\n",
    "        if (csv_name, r['Input.s{}'.format(j)]) not in kv_pairs:\n",
    "            kv_pairs[(csv_name, r['Input.s{}'.format(j)])] = [r[\"Input.o{}\".format(j)], r[\"Answer.A{}\".format(j)], \"None\"]\n",
    "        else:\n",
    "            kv_pairs[(csv_name, r['Input.s{}'.format(j)])][2] = r[\"Answer.A{}\".format(j)]\n",
    "\n",
    "new_kv_pairs = {}\n",
    "for k, v in kv_pairs.iteritems():\n",
    "    if \"Problematic\" not in v:\n",
    "        new_kv_pairs[k] = v\n",
    "        if v[1] == v[2]:\n",
    "            new_kv_pairs[k][0] = v[1]\n",
    "            \n",
    "counter = Counter()\n",
    "p_c = [] \n",
    "for k, v in new_kv_pairs.iteritems():\n",
    "    tmp = set(v)\n",
    "    if len(tmp) == 1:\n",
    "        p_c.append(1)\n",
    "    elif len(tmp) == 2:\n",
    "        p_c.append(1/3.)\n",
    "    else:\n",
    "        p_c.append(0)\n",
    "    \n",
    "    counter.update(v)\n",
    "\n",
    "p_e = []\n",
    "for i, j in counter.items():\n",
    "    p_e.append(j + 0.0)\n",
    "\n",
    "p_e = [_ / sum(p_e) for _ in p_e]\n",
    "p_e = sum(_**2 for _ in p_e)\n",
    "\n",
    "p_c = sum(p_c) / len(p_c)\n",
    "\n",
    "kappa = (p_c - p_e) / (1 - p_e)\n",
    "print \"kappa = {}\".format(kappa)\n",
    "print counter\n",
    "for k, v in new_kv_pairs.iteritems():\n",
    "    print k\n",
    "    print v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_pairs = {}\n",
    "\n",
    "def decide(s1, s2, s3):\n",
    "    s2int = {'Entailed': 1, 'Neutral': 0, 'Contradictory': -1}\n",
    "    avg_score = (s2int[s1] + s2int[s2] + s2int[s3]) // 3.\n",
    "    if avg_score > 0:\n",
    "        return \"Entailed\"\n",
    "    elif avg_score < 0:\n",
    "        return \"Contradictory\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "for k, v in new_kv_pairs.iteritems():\n",
    "    if k[0] not in cleaned_pairs:\n",
    "        cleaned_pairs[k[0]] = [[k[1]], [decide(*v)]]\n",
    "    else:\n",
    "        cleaned_pairs[k[0]][0].append(k[1])\n",
    "        cleaned_pairs[k[0]][1].append(decide(*v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('READY/cleaned.json', 'w') as f:\n",
    "    json.dump(cleaned_pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "import json\n",
    "import csv\n",
    "\n",
    "with open('READY/training_all.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "num = 5\n",
    "count = 0\n",
    "with open('verify_inputs.csv', 'w') as fs:\n",
    "    fields = []\n",
    "    for i in range(1, num + 1):\n",
    "        fields.extend(['url{}'.format(i), 's{}'.format(i), 'o{}'.format(i)])\n",
    "    csvwriter = csv.DictWriter(fs, fieldnames=fields)\n",
    "    csvwriter.writeheader()\n",
    "\n",
    "    buf = {}\n",
    "    for k, v in data.iteritems():\n",
    "        entry = data[k]\n",
    "        for sent, lab in zip(entry[0], entry[1]):\n",
    "            cur = len(buf) // 3 + 1\n",
    "            if cur > num:\n",
    "                csvwriter.writerow(buf)\n",
    "                buf = {}\n",
    "                cur = 1\n",
    "                count += 1\n",
    "            buf['url{}'.format(cur)] = 'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/' + k + '.csv'\n",
    "            buf['s{}'.format(cur)] = sent\n",
    "            if lab == 1:\n",
    "                buf['o{}'.format(cur)] = \"Entailed\"\n",
    "            else:\n",
    "                buf['o{}'.format(cur)] = \"Contradictory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "with open('all_sources/full.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.strip()))\n",
    "    #mapping[str(d['goldAnnotation']['titleId']) + '-' + str(d['tableId'])] = d['goldAnnotation']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "for d in data:\n",
    "    mapping[str(d['pgId']) + '-' + str(d['tableId'])] = d['pgTitle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping.items()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "tiny_mapping = {}\n",
    "for f in os.listdir('data/all_csv/'):\n",
    "    if f.endswith('.csv'):\n",
    "        _, pageid, tableid = f.split('.')[0].split('-')\n",
    "        if pageid + '-' + tableid in mapping:\n",
    "            tiny_mapping[f] = \"https://en.wikipedia.org/wiki/\" + \"_\".join(mapping[pageid + '-' + tableid].split(' '))\n",
    "\n",
    "with open('data/table_to_page.json', 'w') as f:\n",
    "    json.dump(tiny_mapping, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/table_to_page.json', 'r') as f:\n",
    "    tiny_mapping = json.load(f)\n",
    "\n",
    "print len(tiny_mapping)\n",
    "print tiny_mapping['1-18974269-1.html.csv']\n",
    "#print tiny_mapping['1-1007688-1.html.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "import urllib\n",
    "import json\n",
    "\n",
    "with open('data/table_to_page.json', 'r') as f:\n",
    "    tiny_mapping = json.load(f)\n",
    "    \n",
    "new_tiny_mapping = {}\n",
    "for k, v in tiny_mapping.iteritems():\n",
    "    new_tiny_mapping[k] = \"https://en.wikipedia.org/wiki/\" + urllib.quote(v[30:].encode('utf8'))\n",
    "\n",
    "with open('data/table_to_page_new.json', 'w') as f:\n",
    "    json.dump(new_tiny_mapping, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url1</th>\n",
       "      <th>wiki1</th>\n",
       "      <th>topic1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/RFU_Championship</td>\n",
       "      <td>rfu championship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Indiana_State_Sy...</td>\n",
       "      <td>indiana state sycamores football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Tupolev_Tu-154</td>\n",
       "      <td>tupolev tu - 154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Philipp_Eng</td>\n",
       "      <td>philipp eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/United_States_pr...</td>\n",
       "      <td>united states presidential election in nevada ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Block_%28Aus...</td>\n",
       "      <td>the block (australian tv series)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/List_of_Royal_Pa...</td>\n",
       "      <td>list of royal pains episodes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/List_of_Royal_Pa...</td>\n",
       "      <td>list of royal pains episodes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2009_Nature_Vall...</td>\n",
       "      <td>2009 nature valley grand prix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2009%E2%80%9310_...</td>\n",
       "      <td>2009 - 10 philadelphia 76ers season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Iranian_presiden...</td>\n",
       "      <td>iranian presidential election</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2009%E2%80%9310_...</td>\n",
       "      <td>2009 - 10 los angeles lakers season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Queen%27s_Birthd...</td>\n",
       "      <td>queen 's birthday clash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2009%E2%80%9310_...</td>\n",
       "      <td>2009 - 10 european challenge cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Animorphs_%28TV_...</td>\n",
       "      <td>animorphs (tv series)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ivan_Lendl_caree...</td>\n",
       "      <td>ivan lendl career statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/American_Dad%21_...</td>\n",
       "      <td>american dad! (season 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2008%E2%80%9309_...</td>\n",
       "      <td>2008 - 09 arizona state sun devils women 's ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Rizal</td>\n",
       "      <td>rizal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Irish_Sea</td>\n",
       "      <td>irish sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2009%E2%80%9310_...</td>\n",
       "      <td>2009 - 10 new york knicks season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2009%E2%80%9310_...</td>\n",
       "      <td>2009 - 10 atlanta hawks season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2009%E2%80%9310_...</td>\n",
       "      <td>2009 - 10 charlotte bobcats season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2009%E2%80%9310_...</td>\n",
       "      <td>2009 - 10 miami heat season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2009%E2%80%9310_...</td>\n",
       "      <td>2009 - 10 orlando magic season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2009%E2%80%9310_...</td>\n",
       "      <td>2009 - 10 orlando magic season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/John_Newcombe_ca...</td>\n",
       "      <td>john newcombe career statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2009%E2%80%9310_...</td>\n",
       "      <td>2009 - 10 washington wizards season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2009%E2%80%9310_...</td>\n",
       "      <td>2009 - 10 washington wizards season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Senneville%2C_Qu...</td>\n",
       "      <td>senneville , quebec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/USA_Today_All-US...</td>\n",
       "      <td>usa today all - usa high school basketball team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Moroccan_Grand_Prix</td>\n",
       "      <td>moroccan grand prix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Television_in_No...</td>\n",
       "      <td>television in norway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bernard_Ackah</td>\n",
       "      <td>bernard ackah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2974</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/List_of_Daytona_...</td>\n",
       "      <td>list of daytona 500 broadcasters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2975</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2004_World_Junio...</td>\n",
       "      <td>2004 world junior figure skating championships</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bill_Cowher</td>\n",
       "      <td>bill cowher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2977</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kicks_after_the_...</td>\n",
       "      <td>kicks after the siren in australian rules foot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2978</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kicks_after_the_...</td>\n",
       "      <td>kicks after the siren in australian rules foot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2979</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Locomotives_of_t...</td>\n",
       "      <td>locomotives of the southern railway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2980</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Locomotives_of_t...</td>\n",
       "      <td>locomotives of the southern railway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2981</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Locomotives_of_t...</td>\n",
       "      <td>locomotives of the southern railway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2982</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Locomotives_of_t...</td>\n",
       "      <td>locomotives of the southern railway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2983</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2005_World_Figur...</td>\n",
       "      <td>2005 world figure skating championships</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2984</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2007_Georgia_For...</td>\n",
       "      <td>2007 georgia force season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Channel_One_%28U...</td>\n",
       "      <td>channel one (uk and ireland)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2986</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2007%E2%80%9308_...</td>\n",
       "      <td>2007 - 08 belgian first division</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2006_Georgia_For...</td>\n",
       "      <td>2006 georgia force season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Henri_Leconte</td>\n",
       "      <td>henri leconte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Henri_Leconte</td>\n",
       "      <td>henri leconte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2990</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Tripoli_Grand_Prix</td>\n",
       "      <td>tripoli grand prix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2013_Games_of_th...</td>\n",
       "      <td>2013 games of the small states of europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/1985_Games_of_th...</td>\n",
       "      <td>1985 games of the small states of europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Emerich_Dembrovschi</td>\n",
       "      <td>emerich dembrovschi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Houston_Rockets_...</td>\n",
       "      <td>houston rockets all - time roster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Houston_Rockets_...</td>\n",
       "      <td>houston rockets all - time roster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Houston_Rockets_...</td>\n",
       "      <td>houston rockets all - time roster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Houston_Rockets_...</td>\n",
       "      <td>houston rockets all - time roster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Houston_Rockets_...</td>\n",
       "      <td>houston rockets all - time roster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>https://raw.githubusercontent.com/wenhuchen/Ta...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Jerry_Trainor</td>\n",
       "      <td>jerry trainor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url1  \\\n",
       "1000  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1001  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1002  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1003  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1004  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1005  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1006  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1007  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1008  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1009  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1010  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1011  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1012  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1013  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1014  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1015  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1016  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1017  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1018  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1019  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1020  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1021  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1022  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1023  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1024  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1025  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1026  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1027  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1028  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "1029  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "...                                                 ...   \n",
       "2970  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2971  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2972  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2973  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2974  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2975  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2976  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2977  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2978  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2979  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2980  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2981  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2982  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2983  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2984  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2985  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2986  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2987  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2988  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2989  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2990  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2991  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2992  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2993  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2994  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2995  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2996  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2997  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2998  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "2999  https://raw.githubusercontent.com/wenhuchen/Ta...   \n",
       "\n",
       "                                                  wiki1  \\\n",
       "1000     https://en.wikipedia.org/wiki/RFU_Championship   \n",
       "1001  https://en.wikipedia.org/wiki/Indiana_State_Sy...   \n",
       "1002       https://en.wikipedia.org/wiki/Tupolev_Tu-154   \n",
       "1003          https://en.wikipedia.org/wiki/Philipp_Eng   \n",
       "1004  https://en.wikipedia.org/wiki/United_States_pr...   \n",
       "1005  https://en.wikipedia.org/wiki/The_Block_%28Aus...   \n",
       "1006  https://en.wikipedia.org/wiki/List_of_Royal_Pa...   \n",
       "1007  https://en.wikipedia.org/wiki/List_of_Royal_Pa...   \n",
       "1008  https://en.wikipedia.org/wiki/2009_Nature_Vall...   \n",
       "1009  https://en.wikipedia.org/wiki/2009%E2%80%9310_...   \n",
       "1010  https://en.wikipedia.org/wiki/Iranian_presiden...   \n",
       "1011  https://en.wikipedia.org/wiki/2009%E2%80%9310_...   \n",
       "1012  https://en.wikipedia.org/wiki/Queen%27s_Birthd...   \n",
       "1013  https://en.wikipedia.org/wiki/2009%E2%80%9310_...   \n",
       "1014  https://en.wikipedia.org/wiki/Animorphs_%28TV_...   \n",
       "1015  https://en.wikipedia.org/wiki/Ivan_Lendl_caree...   \n",
       "1016  https://en.wikipedia.org/wiki/American_Dad%21_...   \n",
       "1017  https://en.wikipedia.org/wiki/2008%E2%80%9309_...   \n",
       "1018                https://en.wikipedia.org/wiki/Rizal   \n",
       "1019            https://en.wikipedia.org/wiki/Irish_Sea   \n",
       "1020  https://en.wikipedia.org/wiki/2009%E2%80%9310_...   \n",
       "1021  https://en.wikipedia.org/wiki/2009%E2%80%9310_...   \n",
       "1022  https://en.wikipedia.org/wiki/2009%E2%80%9310_...   \n",
       "1023  https://en.wikipedia.org/wiki/2009%E2%80%9310_...   \n",
       "1024  https://en.wikipedia.org/wiki/2009%E2%80%9310_...   \n",
       "1025  https://en.wikipedia.org/wiki/2009%E2%80%9310_...   \n",
       "1026  https://en.wikipedia.org/wiki/John_Newcombe_ca...   \n",
       "1027  https://en.wikipedia.org/wiki/2009%E2%80%9310_...   \n",
       "1028  https://en.wikipedia.org/wiki/2009%E2%80%9310_...   \n",
       "1029  https://en.wikipedia.org/wiki/Senneville%2C_Qu...   \n",
       "...                                                 ...   \n",
       "2970  https://en.wikipedia.org/wiki/USA_Today_All-US...   \n",
       "2971  https://en.wikipedia.org/wiki/Moroccan_Grand_Prix   \n",
       "2972  https://en.wikipedia.org/wiki/Television_in_No...   \n",
       "2973        https://en.wikipedia.org/wiki/Bernard_Ackah   \n",
       "2974  https://en.wikipedia.org/wiki/List_of_Daytona_...   \n",
       "2975  https://en.wikipedia.org/wiki/2004_World_Junio...   \n",
       "2976          https://en.wikipedia.org/wiki/Bill_Cowher   \n",
       "2977  https://en.wikipedia.org/wiki/Kicks_after_the_...   \n",
       "2978  https://en.wikipedia.org/wiki/Kicks_after_the_...   \n",
       "2979  https://en.wikipedia.org/wiki/Locomotives_of_t...   \n",
       "2980  https://en.wikipedia.org/wiki/Locomotives_of_t...   \n",
       "2981  https://en.wikipedia.org/wiki/Locomotives_of_t...   \n",
       "2982  https://en.wikipedia.org/wiki/Locomotives_of_t...   \n",
       "2983  https://en.wikipedia.org/wiki/2005_World_Figur...   \n",
       "2984  https://en.wikipedia.org/wiki/2007_Georgia_For...   \n",
       "2985  https://en.wikipedia.org/wiki/Channel_One_%28U...   \n",
       "2986  https://en.wikipedia.org/wiki/2007%E2%80%9308_...   \n",
       "2987  https://en.wikipedia.org/wiki/2006_Georgia_For...   \n",
       "2988        https://en.wikipedia.org/wiki/Henri_Leconte   \n",
       "2989        https://en.wikipedia.org/wiki/Henri_Leconte   \n",
       "2990   https://en.wikipedia.org/wiki/Tripoli_Grand_Prix   \n",
       "2991  https://en.wikipedia.org/wiki/2013_Games_of_th...   \n",
       "2992  https://en.wikipedia.org/wiki/1985_Games_of_th...   \n",
       "2993  https://en.wikipedia.org/wiki/Emerich_Dembrovschi   \n",
       "2994  https://en.wikipedia.org/wiki/Houston_Rockets_...   \n",
       "2995  https://en.wikipedia.org/wiki/Houston_Rockets_...   \n",
       "2996  https://en.wikipedia.org/wiki/Houston_Rockets_...   \n",
       "2997  https://en.wikipedia.org/wiki/Houston_Rockets_...   \n",
       "2998  https://en.wikipedia.org/wiki/Houston_Rockets_...   \n",
       "2999        https://en.wikipedia.org/wiki/Jerry_Trainor   \n",
       "\n",
       "                                                 topic1  \n",
       "1000                                   rfu championship  \n",
       "1001                   indiana state sycamores football  \n",
       "1002                                   tupolev tu - 154  \n",
       "1003                                        philipp eng  \n",
       "1004  united states presidential election in nevada ...  \n",
       "1005                   the block (australian tv series)  \n",
       "1006                       list of royal pains episodes  \n",
       "1007                       list of royal pains episodes  \n",
       "1008                      2009 nature valley grand prix  \n",
       "1009                2009 - 10 philadelphia 76ers season  \n",
       "1010                      iranian presidential election  \n",
       "1011                2009 - 10 los angeles lakers season  \n",
       "1012                            queen 's birthday clash  \n",
       "1013                   2009 - 10 european challenge cup  \n",
       "1014                              animorphs (tv series)  \n",
       "1015                       ivan lendl career statistics  \n",
       "1016                           american dad! (season 3)  \n",
       "1017  2008 - 09 arizona state sun devils women 's ba...  \n",
       "1018                                              rizal  \n",
       "1019                                          irish sea  \n",
       "1020                   2009 - 10 new york knicks season  \n",
       "1021                     2009 - 10 atlanta hawks season  \n",
       "1022                 2009 - 10 charlotte bobcats season  \n",
       "1023                        2009 - 10 miami heat season  \n",
       "1024                     2009 - 10 orlando magic season  \n",
       "1025                     2009 - 10 orlando magic season  \n",
       "1026                    john newcombe career statistics  \n",
       "1027                2009 - 10 washington wizards season  \n",
       "1028                2009 - 10 washington wizards season  \n",
       "1029                                senneville , quebec  \n",
       "...                                                 ...  \n",
       "2970    usa today all - usa high school basketball team  \n",
       "2971                                moroccan grand prix  \n",
       "2972                               television in norway  \n",
       "2973                                      bernard ackah  \n",
       "2974                   list of daytona 500 broadcasters  \n",
       "2975     2004 world junior figure skating championships  \n",
       "2976                                        bill cowher  \n",
       "2977  kicks after the siren in australian rules foot...  \n",
       "2978  kicks after the siren in australian rules foot...  \n",
       "2979                locomotives of the southern railway  \n",
       "2980                locomotives of the southern railway  \n",
       "2981                locomotives of the southern railway  \n",
       "2982                locomotives of the southern railway  \n",
       "2983            2005 world figure skating championships  \n",
       "2984                          2007 georgia force season  \n",
       "2985                       channel one (uk and ireland)  \n",
       "2986                   2007 - 08 belgian first division  \n",
       "2987                          2006 georgia force season  \n",
       "2988                                      henri leconte  \n",
       "2989                                      henri leconte  \n",
       "2990                                 tripoli grand prix  \n",
       "2991           2013 games of the small states of europe  \n",
       "2992           1985 games of the small states of europe  \n",
       "2993                                emerich dembrovschi  \n",
       "2994                  houston rockets all - time roster  \n",
       "2995                  houston rockets all - time roster  \n",
       "2996                  houston rockets all - time roster  \n",
       "2997                  houston rockets all - time roster  \n",
       "2998                  houston rockets all - time roster  \n",
       "2999                                      jerry trainor  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "t = pandas.read_csv('v5_write_input.csv')\n",
    "length = len(t)\n",
    "\n",
    "t = t.head(3000).tail(2000)\n",
    "#t = t.head(1000)\n",
    "\n",
    "t.to_csv('v5_write_input_1000_3000.csv', index=False)\n",
    "\n",
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
